{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMN_NAME = \"label\"\n",
    "WEIGHT_COLUMN_NAME = None\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    'releaseYear',\n",
    "    'movieRatingCount',\n",
    "    'movieAvgRating',\n",
    "    'movieRatingStddev',\n",
    "    'userRatingCount',\n",
    "    'userAvgRating',\n",
    "    'userRatingStddev'\n",
    "]\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "    'userGenre1',\n",
    "    'userGenre2',\n",
    "    'userGenre3',\n",
    "    'userGenre4',\n",
    "    'userGenre5',\n",
    "    'movieGenre1',\n",
    "    'movieGenre2',\n",
    "    'movieGenre3',\n",
    "]\n",
    "# Maximum number of decision trees. The effective number of trained trees can be smaller if early stopping is enabled.\n",
    "NUM_TREES = 100\n",
    "# Minimum number of examples in a node.\n",
    "MIN_EXAMPLES = 6\n",
    "# Maximum depth of the tree. max_depth=1 means that all trees will be roots.\n",
    "MAX_DEPTH = 5\n",
    "# Ratio of the dataset (sampling without replacement) used to train individual trees for the random sampling method.\n",
    "SUBSAMPLE = 0.65\n",
    "# Control the sampling of the datasets used to train individual trees.\n",
    "SAMPLING_METHOD = \"RANDOM\"\n",
    "# Ratio of the training dataset used to monitor the training. Require to be >0 if early stopping is enabled.\n",
    "VALIDATION_RATIO = 0.1\n",
    "\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    # Cast the categorical features to string.\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        dataframe[feature_name] = dataframe[feature_name].astype(str)\n",
    "\n",
    "\n",
    "def run_experiment(model, train_data, test_data, num_epochs=1, batch_size=None):\n",
    "    train_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "        train_data, label=TARGET_COLUMN_NAME, weight=WEIGHT_COLUMN_NAME\n",
    "    )    \n",
    "    test_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "        test_data, label=TARGET_COLUMN_NAME, weight=WEIGHT_COLUMN_NAME\n",
    "    )\n",
    "    model.fit(train_dataset, epochs=num_epochs, batch_size=batch_size)\n",
    "    _, accuracy, roc, pr = gbt_model.evaluate(test_dataset, verbose=0)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 3)}%,\\n\"\n",
    "          f\"ROC AUC: {round(roc, 3)},\\n\"\n",
    "          f\"PR AUC: {round(pr, 3)}\")\n",
    "\n",
    "\n",
    "def specify_feature_usages():\n",
    "    feature_usages = []\n",
    "\n",
    "    for feature_name in NUMERIC_FEATURE_NAMES:\n",
    "        feature_usage = tfdf.keras.FeatureUsage(\n",
    "            name=feature_name, semantic=tfdf.keras.FeatureSemantic.NUMERICAL\n",
    "        )\n",
    "        feature_usages.append(feature_usage)\n",
    "\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        feature_usage = tfdf.keras.FeatureUsage(\n",
    "            name=feature_name, semantic=tfdf.keras.FeatureSemantic.CATEGORICAL\n",
    "        )\n",
    "        feature_usages.append(feature_usage)\n",
    "\n",
    "    return feature_usages\n",
    "\n",
    "\n",
    "def create_gbt_model(preprocessor=None, postprocessor=None):\n",
    "    gbt_model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        preprocessing=preprocessor,\n",
    "        postprocessing=postprocessor,\n",
    "        num_trees=NUM_TREES,\n",
    "        max_depth=MAX_DEPTH,\n",
    "        min_examples=MIN_EXAMPLES,\n",
    "        subsample=SUBSAMPLE,\n",
    "        validation_ratio=VALIDATION_RATIO,\n",
    "        task=tfdf.keras.Task.CLASSIFICATION,\n",
    "    )\n",
    "\n",
    "    gbt_model.compile(metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                               keras.metrics.AUC(curve=\"ROC\"),\n",
    "                               keras.metrics.AUC(curve=\"PR\")])\n",
    "    return gbt_model\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "\n",
    "    for feature_name in NUMERIC_FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "            name=feature_name, shape=(), dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "            name=feature_name, shape=(), dtype=tf.string\n",
    "        )\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_embedding_encoder(size=None):\n",
    "    inputs = create_model_inputs()\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = sorted(\n",
    "                [str(value) for value in list(train_data[feature_name].unique())]\n",
    "            )\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "            )\n",
    "            # Convert the string input values into integer indices.\n",
    "            value_index = lookup(inputs[feature_name])\n",
    "            # Create an embedding layer with the specified dimensions\n",
    "            vocabulary_size = len(vocabulary)\n",
    "            embedding_size = int(math.sqrt(vocabulary_size))\n",
    "            feature_encoder = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_size\n",
    "            )\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_feature = feature_encoder(value_index)\n",
    "        else:\n",
    "            # Expand the dimensions of the numerical input feature and use it as-is.\n",
    "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "        # Add the encoded feature to the list.\n",
    "        encoded_features.append(encoded_feature)\n",
    "    # Concatenate all the encoded features.\n",
    "    encoded_features = layers.concatenate(encoded_features, axis=1)\n",
    "    # Apply dropout.\n",
    "    encoded_features = layers.Dropout(rate=0.25)(encoded_features)\n",
    "    # Perform non-linearity projection.\n",
    "    encoded_features = layers.Dense(\n",
    "        units=size if size else encoded_features.shape[-1], activation=\"gelu\"\n",
    "    )(encoded_features)\n",
    "    # Create and return a Keras model with encoded features as outputs.\n",
    "    return keras.Model(inputs=inputs, outputs=encoded_features)\n",
    "\n",
    "\n",
    "def create_nn_model(encoder):\n",
    "    inputs = create_model_inputs()\n",
    "    embeddings = encoder(inputs)\n",
    "    output = layers.Dense(units=1, activation=\"sigmoid\")(embeddings)\n",
    "\n",
    "    nn_model = keras.Model(inputs=inputs, outputs=output)\n",
    "    nn_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(\"accuracy\")],\n",
    "    )\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaves_index_to_onehot(prediction):\n",
    "    sample_size, num_trees = prediction.shape\n",
    "    num_leaves = np.max(prediction) + 1\n",
    "    transformed_training_matrix = np.zeros([sample_size, num_trees * num_leaves],\n",
    "                                       dtype=np.int64)\n",
    "    for i in range(0, sample_size):\n",
    "        temp = np.arange(num_trees) * num_leaf + np.array(prediction[i])\n",
    "        transformed_training_matrix[i][temp] += 1\n",
    "    return transformed_training_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logistic_model(num_trees=NUM_TREES, num_leaves=2**(MAX_DEPTH-1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid', input_dim=num_trees*num_leaves))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                          keras.metrics.AUC(curve=\"ROC\"),\n",
    "                          keras.metrics.AUC(curve=\"PR\")])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_samples_file_path = \"../data/trainingSamples.csv\"\n",
    "test_samples_file_path = \"../data/testSamples.csv\"\n",
    "train_data = pd.read_csv(training_samples_file_path)\n",
    "test_data = pd.read_csv(test_samples_file_path)\n",
    "prepare_dataframe(train_data)\n",
    "prepare_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/envs/gbdt/lib/python3.9/site-packages/numpy/core/numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/envs/gbdt/lib/python3.9/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['movieId', 'userId', 'rating', 'timestamp', 'userRatedMovie1', 'userRatedMovie2', 'userRatedMovie3', 'userRatedMovie4', 'userRatedMovie5', 'userAvgReleaseYear', 'userReleaseYearStddev'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 1s 7ms/step - loss: 19.9114 - accuracy: 0.5242\n",
      "Epoch 2/5\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.4273 - accuracy: 0.5385\n",
      "Epoch 3/5\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.1114 - accuracy: 0.5578\n",
      "Epoch 4/5\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.6107 - accuracy: 0.5576\n",
      "Epoch 5/5\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.1643 - accuracy: 0.5699\n",
      "Test accuracy: 65.08%,\n",
      "ROC AUC: 0.709,\n",
      "PR AUC: 0.751\n"
     ]
    }
   ],
   "source": [
    "# GBDT classifier with feature embedding\n",
    "embedding_encoder = create_embedding_encoder(size=64)\n",
    "run_experiment(\n",
    "    create_nn_model(embedding_encoder),\n",
    "    train_data,\n",
    "    test_data,\n",
    "    num_epochs=5,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /var/folders/yn/k_cr94fd387chb6sg2ttcnzc0000gn/T/tmpen1s0x77 as temporary training directory\n",
      "Warning: Model constructor argument batch_size=None not supported. See https://www.tensorflow.org/decision_forests/migration for an explanation about the specificities of TF-DF.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 24-01-05 11:10:01.0157 CST gradient_boosted_trees.cc:1886] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "[WARNING 24-01-05 11:10:01.0158 CST gradient_boosted_trees.cc:1897] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "[WARNING 24-01-05 11:10:01.0158 CST gradient_boosted_trees.cc:1911] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n",
      "WARNING:absl:Model constructor argument batch_size=None not supported. See https://www.tensorflow.org/decision_forests/migration for an explanation about the specificities of TF-DF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/envs/gbdt/lib/python3.9/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['movieId', 'userId', 'rating', 'timestamp', 'userRatedMovie1', 'userRatedMovie2', 'userRatedMovie3', 'userRatedMovie4', 'userRatedMovie5', 'userAvgReleaseYear', 'userReleaseYearStddev'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset read in 0:00:00.892908. Found 88827 examples.\n",
      "Training model...\n",
      "Model trained in 0:00:06.795492\n",
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 24-01-05 11:10:08.8341 CST kernel.cc:1233] Loading model from path /var/folders/yn/k_cr94fd387chb6sg2ttcnzc0000gn/T/tmpen1s0x77/model/ with prefix 3a0e7bd7b6834ee1\n",
      "[INFO 24-01-05 11:10:08.8376 CST abstract_model.cc:1344] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n",
      "[INFO 24-01-05 11:10:08.8376 CST kernel.cc:1061] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 64.354%,\n",
      "ROC AUC: 0.7,\n",
      "PR AUC: 0.746\n"
     ]
    }
   ],
   "source": [
    "gbt_model = create_gbt_model(embedding_encoder)\n",
    "run_experiment(gbt_model, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "    train_data, label=TARGET_COLUMN_NAME, weight=WEIGHT_COLUMN_NAME\n",
    ")\n",
    "test_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "    test_data, label=TARGET_COLUMN_NAME, weight=WEIGHT_COLUMN_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 24-01-05 11:10:09.4878 CST kernel.cc:1233] Loading model from path /var/folders/yn/k_cr94fd387chb6sg2ttcnzc0000gn/T/tmpen1s0x77/model/ with prefix 3a0e7bd7b6834ee1\n",
      "[INFO 24-01-05 11:10:09.4914 CST kernel.cc:1079] Use slow generic engine\n",
      "/Users/haodong/opt/anaconda3/envs/gbdt/lib/python3.9/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['movieId', 'userId', 'rating', 'timestamp', 'userRatedMovie1', 'userRatedMovie2', 'userRatedMovie3', 'userRatedMovie4', 'userRatedMovie5', 'userAvgReleaseYear', 'userReleaseYearStddev'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    }
   ],
   "source": [
    "train_lr = leaves_index_to_onehot(gbt_model.predict_get_leaves(train_dataset))\n",
    "test_lr = leaves_index_to_onehot(gbt_model.predict_get_leaves(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2776/2776 [==============================] - 2s 514us/step - loss: 0.6146 - accuracy: 0.6527 - auc_22: 0.7103 - auc_23: 0.7551\n",
      "Epoch 2/10\n",
      "2776/2776 [==============================] - 1s 468us/step - loss: 0.6007 - accuracy: 0.6676 - auc_22: 0.7292 - auc_23: 0.7713\n",
      "Epoch 3/10\n",
      "2776/2776 [==============================] - 1s 466us/step - loss: 0.5957 - accuracy: 0.6708 - auc_22: 0.7355 - auc_23: 0.7763\n",
      "Epoch 4/10\n",
      "2776/2776 [==============================] - 1s 475us/step - loss: 0.5924 - accuracy: 0.6736 - auc_22: 0.7393 - auc_23: 0.7798\n",
      "Epoch 5/10\n",
      "2776/2776 [==============================] - 1s 485us/step - loss: 0.5908 - accuracy: 0.6754 - auc_22: 0.7409 - auc_23: 0.7811\n",
      "Epoch 6/10\n",
      "2776/2776 [==============================] - 1s 472us/step - loss: 0.5895 - accuracy: 0.6768 - auc_22: 0.7425 - auc_23: 0.7825\n",
      "Epoch 7/10\n",
      "2776/2776 [==============================] - 1s 478us/step - loss: 0.5885 - accuracy: 0.6765 - auc_22: 0.7432 - auc_23: 0.7834\n",
      "Epoch 8/10\n",
      "2776/2776 [==============================] - 1s 494us/step - loss: 0.5874 - accuracy: 0.6786 - auc_22: 0.7446 - auc_23: 0.7848\n",
      "Epoch 9/10\n",
      "2776/2776 [==============================] - 1s 475us/step - loss: 0.5868 - accuracy: 0.6794 - auc_22: 0.7453 - auc_23: 0.7850\n",
      "Epoch 10/10\n",
      "2776/2776 [==============================] - 1s 498us/step - loss: 0.5867 - accuracy: 0.6782 - auc_22: 0.7454 - auc_23: 0.7853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2906b7850>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = create_logistic_model()\n",
    "lr_model.fit(train_lr, train_data.label, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 65.28%,\n",
      "ROC AUC: 0.71,\n",
      "PR AUC: 0.749\n"
     ]
    }
   ],
   "source": [
    "_, accuracy, roc, pr = lr_model.evaluate(test_lr, test_data.label, verbose=0)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%,\\n\"\n",
    "      f\"ROC AUC: {round(roc, 3)},\\n\"\n",
    "      f\"PR AUC: {round(pr, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
